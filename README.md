# udacity-detecting-model-drift

Change is inevitable. When we deploy ML models to production, we have to be prepared for "model drift," which occurs when conditions in the world change and cause our model performance to get worse.

In this exercise, you'll create a script that analyzes model scores to check for model drift.

You'll perform three different tests that each check for model drift:

a simple, raw comparison test
a parametric significance test
a non-parametric outlier test
We've provided a starter file called modeldrift.py in the L3 directory of your workspace. You can write code for your solution in that file.

